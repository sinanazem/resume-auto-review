EXAMPLE_JOB_DESCRIPTION = """
Full job description
Position: Data Engineer

Client: Insurance Client

Location: Remote (Canada)

Duration: 4 Months (With possible extension)

Language: English

Background: Our clients are seeing the services of a Senior Data Engineer to provide Engineering Leadership for the design and build of data products that collect and store large datasets and lead the design and build of data platforms that underpin and accelerate the build of data products. You ensure that data products comply with the data access policies and rules that the governance team implements and implements robust security measures to protect sensitive information. You are expected to be a seasoned professional with deep understanding of Azure cloud infrastructure, automation principles and has a security-first mindset.

Qualifications:

· Experience coding with terraform, Python and Pyspark.

· Hands-on experience with the following Azure PaaS: Azure Data Lake Storage (ADLS), Azure Databricks, Azure Data Factory (ADF), Azure Synapse, Event Hub, APIM, Azure Key Vault, Azure SQL, and Purview. Have a good understanding of the backup, disaster recovery, and data recovery strategy and execution with the above services.

· Good to have knowledge/experience with Microsoft Fabric (SaaS) and Gen AI-related technology including Azure ML, Azure OpenAI and Databricks.

· Good knowledge of Azure infrastructure such as cloud security and networking and experience in implementing best practices.

Responsibilities:

· Create and maintain detailed architectural diagrams to support business requirements.

· Lead the build of Data Pipelines templates: Lead a team of engineers in building robust data pipeline templates to automate the future development of data pipelines.

· Data Integration and Curation: Lead the team in configuring the pipeline templates to integrate data from multiple sources and in implementing the curation logic in the medallion architecture.

· Data Quality Reconciliation: Lead the implementation of data quality libraries and reconciliation methodologies to ensure quality and completeness of data.

· Monitoring and Troubleshooting: Define and lead the implementation of pipeline monitoring systems and setup an automated support framework that alerts the support staff automatically.

· Performance Optimization: Define and implement automated process to generate alerts on failures or performance degradation of data products with a view of taking proactive steps in resolving issues before they become evident.

· Documentation and Maintenance: Define, lead and enforce a documentation system to capture the state of the existing implementation, and to ensure ongoing maintenance and updates.

· Process Improvement: Identify and implement engineering best practices and process improvements to enhance the efficiency and effectiveness of the data engineering team.

Job Type: Fixed term contract
Contract length: 4 months

Pay: $64.98-$69.38 per hour

Expected hours: 37.5 per week

Flexible language requirement:

French not required
Schedule:

Monday to Friday
Application question(s):

Describe your coding experience with Python and Pyspark?
Describe your experience with experience with the following Azure PaaS: Azure Data Lake Storage (ADLS), Azure Databricks, Azure Data Factory (ADF), Azure Synapse, Event Hub, APIM, Azure Key Vault, Azure SQL, and Purview?
Describe your working experience with Microsoft Fabric (SaaS) and Gen AI-related technology including Azure ML, Azure OpenAI and Databricks.
Work Location: Remote

Expected start date: 2024-09-01
"""